# a_core.py (updated) - fetch endpoints per-metric and fully parallel pipeline
"""
fetch_data_for_pipeline ÅŸunlarÄ± yapar:
required_endpoints = ["klines"] (sadece klines)
fetcher.fetch_all_for_symbol("BTCUSDT", ["klines"])
Binance'den klines verisini Ã§eker
_klines_to_dataframe ile DataFrame'e Ã§evirir:

calculate_metrics> classical.py'deki ema

â€œYÃ¼kselme ihtimaliâ€ skoru (Composite Alpha)
ALPHA_SCORE =
 + 0.30 * trend
 + 0.20 * core
 + 0.15 * mom
 + 0.15 * sentiment
 + 0.10 * flow
 - 0.10 * risk
 
 Yorum
Trend + Core ana motor
Volmom = erken ivme
Sentiment + Flow = yakÄ±t
Risk = frene basan el
Bu skor handlerâ€™da deÄŸil coreâ€™da Ã¼retilmeli



ðŸ”¥ beyin: (alphax + core + risk)

ðŸ§  alphax
trend + mom + sentiment + flow - risk
â†’ â€œYÃ¼kselme / dÃ¼ÅŸme isteÄŸi var mÄ±?â€

ðŸ§± core
trend + vol + regim + risk
â†’ â€œBu istek yapÄ±sal olarak saÄŸlÄ±klÄ± mÄ±?â€

ðŸš¨ risk
â†’ â€œHer ÅŸey gÃ¼zel ama patlar mÄ±?â€

âœ… LONG iÃ§in ideal senaryo
alphax > +0.35
core   > +0.20
risk   < 0.30


âŒ Sahte yÃ¼kseliÅŸ (Ã§ok kritik!)
alphax > +0.40
core   < 0
risk   > 0.50


Trend-follow uygun mu?
complexity < 0.4
vol        < 0.5
regim      > 0

Chop / range ortamÄ±
ðŸ“Œ Bu kombinasyon olmadan trend sinyali kullanmak kÃ¶r uÃ§uÅŸ olur.
complexity > 0.6
regim      < 0


Trade aÃ§Ä±labilir mi?
microstructure > 0
liqrisk        < 0.3

-Ã¶zet--
| AmaÃ§                          | Gerekli Kombinasyon        |
| ----------------------------- | -------------------------- |
| **Ana yÃ¶n kararÄ±**            | `alphax + core + risk`     |
| **Trend ortamÄ± mÄ±?**          | `complexity + regim + vol` |
| **Scalp teyidi**              | `trend + mom + order`   |
| **Sentiment tuzaÄŸÄ± filtresi** | `sentflow + trend + risk`  |
| **Trade izni**                | `microstructure + liqrisk` |

| Composite | CevapladÄ±ÄŸÄ± Soru          |
| --------- | ------------------------- |
| trend     | *YÃ¶n var mÄ±?*             |
| mom    | *hÄ±z:YÃ¶n hÄ±zlanÄ±yor mu?*      |
| vol       | *rejim:Ortam ne kadar oynak?*   |
| sentiment | *aÄŸÄ±rlÄ±k:Pozisyonlanma ne diyor?* |
| risk      | *Bu iÅŸ patlar mÄ±?*        |



"""

from __future__ import annotations
import asyncio
import logging
import math
import ast
import concurrent.futures
import os
from typing import Any, Dict, List, Optional, Union, Tuple, Callable, Set
from functools import partial

from analysis.metricresolver import get_default_resolver
from utils.binance_api.binance_a import BinanceAggregator

import pandas as pd
import numpy as np
from datetime import datetime

# ------------------------------------------------------------
# Logger
# ------------------------------------------------------------
logger = logging.getLogger("analysis.core")
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s"))
    logger.addHandler(h)
# logger.setLevel(logging.INFO)
logger.setLevel(logging.DEBUG)

# ------------------------------------------------------------
# Globals & Constants
# ------------------------------------------------------------
DEFAULT_DATA_MODEL = "pandas"

# ------------------------------------------------------------
# COMPOSITES / MACROS maps 
# ------------------------------------------------------------

COMPOSITES = {
    # âœ… binance api ile baÅŸarÄ±lÄ±
    "trend": { # Sadece directional bias
        "depends": ["ema", "macd", "rsi", "stochastic_oscillator"],
        "formula": "0.30*ema + 0.30*macd + 0.20*rsi + 0.20*stochastic_oscillator",
    },
    "mom": { # Hareket var mÄ± ve gÃ¼Ã§leniyor mu
        "depends": ["roc", "adx", "atr"],
        "formula": "0.45*roc + 0.35*adx - 0.20*atr",
    },
    "vol": { # Bu piyasa trend taÅŸÄ±r mÄ±
        "depends": ["historical_volatility", "garch_1_1", "hurst_exponent"],
        "formula": "0.40*historical_volatility + 0.35*garch_1_1 + 0.25*(1 - hurst_exponent)",
    },
    
    # âœ… âš ï¸ Hesaplanabilir, yaklaÅŸÄ±k bilgi veriri, gerÃ§ek anlamlÄ±lÄ±k sÄ±nÄ±rlÄ±.
    "sentiment": {
        "depends": ["funding_rate", "funding_premium", "oi_trend"],
        "formula": "0.35*funding_rate + 0.25*funding_premium + 0.40*oi_trend",
    },
    "risk": {
        "depends": ["volatility_risk", "liquidity_depth_risk", "price_impact_risk"],
        "formula": "0.40*volatility_risk + 0.35*liquidity_depth_risk + 0.25*price_impact_risk",
    },
    "regim": {
        "depends": ["advance_decline_line", "volume_leadership", "performance_dispersion"],
        "formula": "0.45*advance_decline_line + 0.35*volume_leadership + 0.20*performance_dispersion",
    },
    "entropy": {# entropy_fractal
        "depends": ["entropy_index", "fractal_dimension_index_fdi", "hurst_exponent", "variance_ratio_test"],
        "formula": "0.35*entropy_index + 0.25*fractal_dimension_index_fdi - 0.25*hurst_exponent - 0.15*variance_ratio_test",
    },
    
    # âš ï¸ anlamsÄ±z metrikler, yetersiz veri nedeniyle
    "liqu": {
        "depends": ["liquidity_density","microprice_deviation"],
        "formula": "0.5*liquidity_density+0.5*microprice_deviation",
    },
    "liqrisk": {
        "depends": ["liquidity_density", "liquidity_gaps"], # âŒ "market_impact", "cascade_risk"
        "formula": "0.5*liquidity_density - 0.2*liquidity_gaps",
    },
    
    # âŒ hesaplanamaz binance api ile
    "order": {
        "depends": ["ofi", "cvd", "microprice_deviation"], # "taker_dominance_ratio"
        "formula": "0.45*ofi + 0.35*cvd  + 0.20*microprice_deviation",
    },
    "flow": {
        "depends": ["etf_net_flow", "exchange_netflow", "stablecoin_flow"],
        "formula": "0.4*etf_net_flow - 0.3*exchange_netflow + 0.3*stablecoin_flow",
    },

}	
# Binance API ile doÄŸrudan elde edilemeyenler
# market_impact, depth_elasticity, taker_dominance_ratio (ham veriden tÃ¼retilir ama direkt verilmez)
# âš ï¸ garch_1_1, hurst_exponent, fdi, variance_ratio_test, fractal_dimension_index_fdi




MACROS = {	
    "core": {
        "depends": ["trend", "vol", "regim", "risk"],
        "formula": "0.35*trend + 0.25*vol + 0.25*regim + 0.15*risk",
    },
    "alphax":{
        "depends": ["trend","mom","sentiment","flow","risk"],
        "formula": "0.35*trend + 0.25*mom + 0.20*sentiment + 0.15*flow - 0.15*risk", 
    },
    "sentri": { # Sinyal > 0 long, = 0 bekle, <0 short
        "depends": ["sentiment", "entropy", "regim", "liquidity_density", "risk"],
        "formula": "0.24*sentiment + 0.18*entropy + 0.2*regim+ 0.2*liquidity_density - 0.18*risk",
    },
    
    "coreliq": {
        "depends": ["trend", "vol", "regim", "risk", "liqu"],
        "formula": "0.27*trend + 0.20*vol + 0.20*regim + 0.18*risk + 0.15*liqu",
    },
    "complexity": {
        "depends": ["entropy", "vol"],
        "formula": "0.6*entropy + 0.4*vol",
    },
    "sentflow": {
        "depends": ["sentiment", "flow"],
        "formula": "0.55*sentiment + 0.45*flow",
    },
    "microstructure": {
        "depends": ["liqu", "liqrisk", "order"],
        "formula": "0.4*liqu+ 0.35*liqrisk + 0.25*order",
    },
}

# ------------------------------------------------------------
# Formula compile helpers (unchanged)
# ------------------------------------------------------------
_ALLOWED_NODES = {
    ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load, ast.Add,
    ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd, ast.Name,
    ast.Constant, ast.Mod, ast.FloorDiv, ast.Call
}

_formula_compile_cache: Dict[str, Any] = {}
_formula_compile_lock = asyncio.Lock()

def _validate_ast(node: ast.AST) -> None:
    for n in ast.walk(node):
        if type(n) not in _ALLOWED_NODES:
            raise ValueError(f"Disallowed AST node: {type(n).__name__}")

async def _get_compiled_formula(formula: str):
    if not formula:
        return None
    if formula in _formula_compile_cache:
        return _formula_compile_cache[formula]
    async with _formula_compile_lock:
        if formula in _formula_compile_cache:
            return _formula_compile_cache[formula]
        try:
            tree = ast.parse(formula, mode="eval")
            _validate_ast(tree)
            code = compile(tree, "<formula>", "eval")
            _formula_compile_cache[formula] = code
            return code
        except Exception as e:
            logger.warning(f"Formula compile failed: {formula} -> {e}")
            _formula_compile_cache[formula] = None
            return None

def evaluate_compiled_formula(code_obj, ctx: Dict[str, float]) -> float:
    if code_obj is None: return 0.0
    # ctx iÃ§indeki nan deÄŸerlerini 0.0 ile temizle
    safe_ctx = {k: (v if not math.isnan(v) else 0.0) for k, v in ctx.items()}
    try:
        val = eval(code_obj, {"__builtins__": {}}, safe_ctx)
        return float(val) if val is not None else 0.0
    except Exception:
        return 0.0


# ------------------------------------------------------------
# Utilities from original file: resolve_scores_to_metrics, extract_final_value, etc.
# Keep them the same as original. (Copy/paste from your original a_core.py)
def resolve_scores_to_metrics(requested_scores: List[str], COMPOSITES: Dict = None, MACROS: Dict = None) -> Dict[str, List[str]]:
    COMPOSITES = COMPOSITES or {}
    MACROS = MACROS or {}
    out = {}
    for score_name in requested_scores:
        metrics = []
        if score_name in COMPOSITES:
            metrics.extend(COMPOSITES[score_name].get("depends", []))
        elif score_name in MACROS:
            for dep in MACROS[score_name].get("depends", []):
                if dep in COMPOSITES:
                    metrics.extend(COMPOSITES[dep].get("depends", []))
        out[score_name] = sorted(set(metrics))
    return out

# (extract_final_value function unchanged - paste from original)

def extract_final_value(raw_result: Any, metric_name: str) -> float:
    if raw_result is None:
        return float("nan")
    
    try:
        # 1. Zaten float/int ise direkt dÃ¶ndÃ¼r
        if isinstance(raw_result, (int, float, np.number)):
            return float(raw_result)  # â† BU KESÄ°NLÄ°KLE Ã‡ALIÅžACAK
        
        # 2. Pandas Series
        if isinstance(raw_result, pd.Series):
            if raw_result.empty:
                return float("nan")
            return float(raw_result.iat[-1])
        
        # 3. DataFrame
        if isinstance(raw_result, pd.DataFrame):
            if raw_result.empty:
                return float("nan")
            for col in ['value', 'score', metric_name, 'result']:
                if col in raw_result.columns:
                    try:
                        return float(raw_result[col].iat[-1])
                    except Exception:
                        continue
            try:
                return float(raw_result.iat[-1, 0])
            except Exception:
                return float("nan")
        if isinstance(raw_result, (list, tuple, np.ndarray)):
            try:
                if isinstance(raw_result, np.ndarray):
                    if raw_result.size == 0:
                        return float("nan")
                    return float(np.asarray(raw_result).flat[-1])
                else:
                    if len(raw_result) == 0:
                        return float("nan")
                    return float(raw_result[-1])
            except Exception:
                return float("nan")
        if isinstance(raw_result, dict):
            for key in ('value', 'score', metric_name, 'result', 'data'):
                if key in raw_result:
                    try:
                        return float(raw_result[key])
                    except Exception:
                        continue
            for v in raw_result.values():
                if isinstance(v, (int, float, np.number)):
                    return float(v)
            return float("nan")
        if isinstance(raw_result, (int, float, np.number)):
            return float(raw_result)
        if isinstance(raw_result, str):
            try:
                return float(raw_result)
            except Exception:
                return float("nan")
        return float(str(raw_result))
    except Exception:
        return float("nan")


# ------------------------------------------------------------
# ThreadPool executor (shared) - Global Seviye
# ------------------------------------------------------------

_CPU = os.cpu_count() or 2
_DEFAULT_MAX_WORKERS = min(max(4, _CPU * 2), 20)
_global_executor = concurrent.futures.ThreadPoolExecutor(max_workers=_DEFAULT_MAX_WORKERS)


_CPU_EXECUTOR = concurrent.futures.ProcessPoolExecutor(
    max_workers=os.cpu_count()
)
_IO_EXECUTOR = _global_executor


def run_sync_metric(
    fn: Callable,
    inp,
    params: Dict,
    metric_name: str
) -> Tuple[str, float]:
    """
    Global seviyeye taÅŸÄ±nan senkron metrik Ã§alÄ±ÅŸtÄ±rÄ±cÄ±.
    ArtÄ±k pickle edilebilir (ProcessPool iÃ§in uygun).
    """
    try:
        # Not: extract_final_value fonksiyonunun da globalde tanÄ±mlÄ± olduÄŸundan emin olun
        raw = fn(inp, **params)
        val = extract_final_value(raw, metric_name)
        
        # NaN kontrolÃ¼ ve kÄ±rpma
        if math.isnan(val):
            return metric_name, float("nan")
            
        return metric_name, float(np.clip(val, -1, 1))
    except Exception as e:
        # Alt processlerdeki hatalarÄ± ana sÃ¼rece bildirmek iÃ§in log veya hata dÃ¶nÃ¼yoruz
        return metric_name, float("nan")

async def run_async_metric(
    fn: Callable,
    inp,
    params: Dict,
    metric_name: str
) -> Tuple[str, float]:
    """Asenkron metrik Ã§alÄ±ÅŸtÄ±rÄ±cÄ± (Global scope)"""
    try:
        raw = await fn(inp, **params)
        val = extract_final_value(raw, metric_name)
        
        if math.isnan(val):
            return metric_name, float("nan")
            
        return metric_name, float(np.clip(val, -1, 1))
    except Exception as e:
        return metric_name, float("nan")
        
# ------------------------------------------------------------
# Data preparation (unchanged)
# ------------------------------------------------------------

def prepare_data(data: pd.DataFrame, def_info: Dict) -> Any:
    if data is None or data.empty:
        return pd.DataFrame()

    data_model = def_info.get("data_model", "pandas")
    required_cols = def_info.get("required_columns", []) or []

    if required_cols:
        selected_cols = {}

        for col in required_cols:
            # 1ï¸âƒ£ Direkt varsa
            if col in data.columns:
                selected_cols[col] = data[col]
                continue

            # 2ï¸âƒ£ suffixâ€™li kolonlarÄ± ara (close__klines gibi)
            matches = [c for c in data.columns if c.startswith(col + "__")]
            if matches:
                # ilk bulunanÄ± al (klines genelde tek olur)
                selected_cols[col] = data[matches[0]]

        if not selected_cols:
            return pd.DataFrame()

        selected = pd.DataFrame(selected_cols, index=data.index)
    else:
        selected = data

    if data_model == "numpy":
        try:
            return selected.to_numpy()
        except Exception:
            return selected

    if data_model == "polars":
        try:
            import polars as pl
            return pl.from_pandas(selected)
        except Exception:
            return selected

    return selected



# ------------------------------------------------------------
# Metric execution (unchanged)
# ------------------------------------------------------------
# sadece debug, debugsuz olanÄ± altta
# max_workers: int = None
# Parallel, NaN-safe, CPU-aware metric execution engine.

async def calculate_metrics(
    data: pd.DataFrame,
    metric_defs: Dict[str, Dict],
    max_workers: int = None
) -> Dict[str, float]:
    logger.debug(f"calculate_metrics called with {len(metric_defs)} metrics")

    if data is None or data.empty or not metric_defs:
        logger.warning("No data or metric definitions")
        return {}

    loop = asyncio.get_running_loop()
    results: Dict[str, float] = {}
    tasks: List[asyncio.Future] = []

    # Executor'lar (BunlarÄ±n yukarÄ±da tanÄ±mlandÄ±ÄŸÄ±nÄ± varsayÄ±yoruz)
    CPU_EXECUTOR = _CPU_EXECUTOR
    IO_EXECUTOR = _global_executor

    for name, def_info in metric_defs.items():
        func = def_info.get("function")
        params = def_info.get("default_params", {}) or {}
        exec_type = def_info.get("execution_type", "sync")
        metadata = def_info.get("metadata", {}) or {}

        if func is None:
            results[name] = float("nan")
            continue

        try:
            input_data = prepare_data(data, def_info)
        except Exception as e:
            logger.debug(f"prepare_data failed: {name} â†’ {e}")
            results[name] = float("nan")
            continue

        # Minimum bar kontrolÃ¼
        min_bars = metadata.get("min_bars", 1)
        if hasattr(input_data, "__len__") and len(input_data) < min_bars:
            results[name] = float("nan")
            continue

        # --- ASYNC GÃ–REVLER ---
        if exec_type == "async":
            tasks.append(
                asyncio.create_task(
                    run_async_metric(func, input_data, params, name)
                )
            )
            continue

        # --- SYNC GÃ–REVLER (Executor ile) ---
        category = metadata.get("category", "")
        # Kategoriye gÃ¶re doÄŸru executor seÃ§imi
        executor = CPU_EXECUTOR if category in ("advanced", "volatility") else IO_EXECUTOR

        # run_in_executor artÄ±k global 'run_sync_metric' fonksiyonunu sorunsuzca pickle edebilir
        future = loop.run_in_executor(
            executor,
            run_sync_metric,
            func,
            input_data,
            params,
            name
        )
        tasks.append(future)

    # --- SONUÃ‡LARI TOPLA ---
    if tasks:
        gathered = await asyncio.gather(*tasks, return_exceptions=True)
        for item in gathered:
            if isinstance(item, tuple) and len(item) == 2:
                k, v = item
                results[k] = v
            elif isinstance(item, Exception):
                # HatalarÄ± gÃ¶rÃ¼nÃ¼r hale getirin
                logger.error(f"Kritik Metrik HatasÄ±: {item}")

    return results
    
# ------------------------------------------------------------
# ------------------------------------------------------------
# Composite/Macro calculation (unchanged)
# ------------------------------------------------------------

# debug
async def calculate_formula_scores(
    source_values: Dict[str, float],
    definitions: Dict[str, dict]
) -> Dict[str, float]:

    out = {}
    formula_map: Dict[str, Any] = {}

    logger.debug(f"calculate_formula_scores SOURCE VALUES: {source_values}")


    # 1ï¸âƒ£ Compile (cached)
    for name, info in definitions.items():
        
        # DEBUG: VOL iÃ§in Ã¶zel log
        if name == "vol":
            deps = info.get("depends", [])
            logger.debug(f"DEBUG VOL calculation - deps: {deps}")
            logger.debug(
                f"DEBUG VOL values - atr: {source_values.get('atr')}, "
                f"hist_vol: {source_values.get('historical_volatility')}, "
                f"garch: {source_values.get('garch_1_1')}, "
                f"hurst: {source_values.get('hurst_exponent')}"
            )
        code = await _get_compiled_formula(info.get("formula"))
        formula_map[name] = (code, info.get("depends", []))

    # 2ï¸âƒ£ Evaluate (NaN-robust)
    for name, (code, deps) in formula_map.items():
        if not code or not deps:
            out[name] = float("nan")
            continue

        values = {}
        valid_weights = 0.0

        for dep in deps:
            v = source_values.get(dep, float("nan"))
            if not math.isnan(v):
                values[dep] = float(v)
                valid_weights += 1
            else:
                values[dep] = 0.0  # NaN kÄ±rÄ±cÄ±

        if valid_weights == 0:
            out[name] = float("nan")
            continue

        # regime factor (safe)
        if "hurst_exponent" in values:
            h = values["hurst_exponent"]
            if not math.isnan(h):
                values["regime_factor"] = max(-1.0, min(1.0, (h - 0.5) * 2.0))

        out[name] = evaluate_compiled_formula(code, values)

    return out





# ------------------------------------------------------------
# Data fetcher: fetch multiple endpoints per-symbol in parallel
# ------------------------------------------------------------
class BinanceDataFetcher:
    """Wrapper to fetch multiple endpoints for a symbol in parallel and merge into one DataFrame."""
    def __init__(self):
        self.aggregator = None

    async def initialize(self):
        if self.aggregator is None:
            self.aggregator = await BinanceAggregator.get_instance()
        return self.aggregator

    def normalize_depth(self, data, symbol, top_n=20):
        """
        RAW depth verisini KORU, sadece formatÄ±nÄ± dÃ¼zenle.
        risk.py'nin beklediÄŸi [side, price, size] formatÄ±na Ã§evir.
        """
        bids = data.get("bids", [])[:top_n]
        asks = data.get("asks", [])[:top_n]

        if not bids or not asks:
            return pd.DataFrame()

        rows = []
        
        # Bids (highest to lowest)
        for i, (price_str, size_str) in enumerate(bids):
            try:
                rows.append({
                    'level': i,
                    'side': 'bid',
                    'price': float(price_str),
                    'size': float(size_str),
                    'symbol': symbol,
                    'timestamp': pd.Timestamp.utcnow()
                })
            except (ValueError, TypeError):
                continue
        
        # Asks (lowest to highest)
        for i, (price_str, size_str) in enumerate(asks):
            try:
                rows.append({
                    'level': i + len(bids),
                    'side': 'ask',
                    'price': float(price_str),
                    'size': float(size_str),
                    'symbol': symbol,
                    'timestamp': pd.Timestamp.utcnow()
                })
            except (ValueError, TypeError):
                continue
        
        if not rows:
            return pd.DataFrame()
        
        df = pd.DataFrame(rows)
        
        # DoÄŸru sÄ±ralama
        df_bids = df[df['side'] == 'bid'].sort_values('price', ascending=False)
        df_asks = df[df['side'] == 'ask'].sort_values('price', ascending=True)
        
        result = pd.concat([df_bids, df_asks], ignore_index=True)
        result.set_index('timestamp', inplace=True)
        
        return result
    
    
    # ================================
    # ðŸ”¥ Yeni sistem â€” tek doÄŸru endpoint Ã§aÄŸrÄ±sÄ±
    # ================================
    async def fetch_endpoint_with_params(self, symbol: str, endpoint_name: str, params: Dict) -> pd.DataFrame:
        
        await self.initialize()
        try:
            data = await self.aggregator.get_public_data(
                endpoint_name=endpoint_name,
                **params
            )
            
            if endpoint_name == "klines":
                return _klines_to_dataframe(data, symbol)
                
            elif endpoint_name == "depth":   # ðŸ”¥ BURADA DEÄžÄ°ÅžTÄ°RDÄ°K
                # ArtÄ±k RAW depth verisini formatlayarak dÃ¶ndÃ¼rÃ¼yoruz
                return self.normalize_depth(data, symbol, top_n=20)
             
            else:
                return _endpoint_to_dataframe(data, symbol, endpoint_name)
        except Exception as e:
            logger.warning(f"fetch_endpoint failed: {symbol} {endpoint_name} -> {e}")
            return pd.DataFrame()
        
    
    # ================================
    # ðŸ”¥ TÃ¼m endpointleri paralel Ã§ek
    # ================================

    async def fetch_all_for_symbol(self, symbol: str, endpoint_params: Dict[str, Dict]):
        tasks = {
            ep: asyncio.create_task(
                self.fetch_endpoint_with_params(symbol, ep, params)
            )
            for ep, params in endpoint_params.items()
        }

        results: Dict[str, pd.DataFrame] = {}
        for ep, task in tasks.items():
            try:
                df = await task
                results[ep] = df if not df.empty else pd.DataFrame()
            except Exception as e:
                logger.error(f"Failed to fetch {ep} for {symbol}: {e}")
                results[ep] = pd.DataFrame()

        # Merge all DataFrames
        merged = None
        for ep, df in results.items():
            if df is None or df.empty:
                continue

            df_copy = df.copy()
            cols = [c for c in df_copy.columns if c != "symbol"]
            rename_map = {c: f"{c}__{ep}" for c in cols}
            df_copy = df_copy.rename(columns=rename_map)

            # timestamp varsa DatetimeIndex yap
            if "timestamp" in df_copy.columns:
                df_copy["timestamp"] = pd.to_datetime(
                    df_copy["timestamp"], unit="ms", errors="coerce", utc=True
                )
                df_copy = df_copy.set_index("timestamp")
            else:
                # timestamp yoksa index reset ve tek seviyeye dÃ¼ÅŸÃ¼r
                df_copy = df_copy.reset_index()
                df_copy.index.name = "timestamp"

            # Merge iÃ§in tÃ¼m DataFrame'leri tek seviyeli index yap
            if not isinstance(df_copy.index, pd.DatetimeIndex):
                df_copy.index = pd.Index(df_copy.index, name="timestamp")

            if merged is None:
                merged = df_copy
            else:
                # ArtÄ±k farklÄ± seviyeler hatasÄ± olmayacak
                merged = pd.merge(
                    merged, df_copy, left_index=True, right_index=True, how="outer"
                )

                # Fazla symbol kolonlarÄ±nÄ± temizle
                sym_cols = [c for c in merged.columns if c == "symbol" or c.endswith("__symbol")]
                if len(sym_cols) > 1:
                    for c in sym_cols[1:]:
                        merged.drop(columns=[c], inplace=True, errors="ignore")

        if merged is not None:
            merged.sort_index(inplace=True)
        else:
            merged = pd.DataFrame()

        return merged


# ================================
# ðŸ”¥ fetch_data_for_pipeline â€” endpoint param Ã¼retme & fetch yÃ¶netimi
# ================================
def filter_healthy_symbols(results):
    healthy = {}

    for sym, data in results.items():
        s = data["scores"]

        if s["LIQRISK"] > 0.5:
            continue
        if s["ENTROPY"] > 0.8:
            continue
        if s["REGIM"] < -0.3:
            continue
        if s["VOL"] > 0.6 and s["TREND"] <= 0:
            continue

        healthy[sym] = data

    return healthy


async def get_top_volume_symbols(count: int = 10):
    """
    En yÃ¼ksek hacimli sembolleri filtreler ve getirir.
    Performans iÃ§in Ã¶nce ilk 30-40 tanesini ayÄ±rÄ±r, 
    sonra iÃ§inden istenen n tanesini dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        from utils.binance_api.binance_a import BinanceAggregator

        # 1. TÃ¼m 24s ticker verilerini Ã§ek
        aggregator = await BinanceAggregator.get_instance()
        
        all_tickers = await aggregator.get_public_data(
            endpoint_name="ticker_24hr"
        )
        
        if not all_tickers:
            return ["BTCUSDT", "ETHUSDT", "BNBUSDT"]  # fallback

        # 2. Sadece USDT Ã§iftlerini ve 'saÄŸlÄ±klÄ±' olanlarÄ± filtrele
        # (UP, DOWN, BULL, BEAR gibi kaldÄ±raÃ§lÄ± tokenlarÄ± eliyoruz)      
        excluded_keywords = ["UP", "DOWN", "BULL", "BEAR"]
        stable_coins = ["USDC", "FDUSD", "USD1", "TUSD", "DAI", "USDP", "EUR", "AEUR", "PAXG"]
        valid_pairs = []

        for ticker in all_tickers:
            symbol = ticker.get('symbol', '')
            if not symbol:
                continue
                
            # USDT ile bitiyor mu?
            # KaldÄ±raÃ§lÄ± token iÃ§ermiyor mu?
            # DiÄŸer stable coin'leri iÃ§ermiyor mu?
            if (symbol.endswith('USDT') and 
                not any(k in symbol for k in excluded_keywords) and
                not any(s in symbol for s in stable_coins)):
                valid_pairs.append(ticker)

        # 3. Hacme (quoteVolume) gÃ¶re bÃ¼yÃ¼kten kÃ¼Ã§Ã¼ÄŸe sÄ±rala
        # quoteVolume = USDT cinsinden toplam hacim
        sorted_pairs = sorted(
            valid_pairs, 
            key=lambda x: float(x.get('quoteVolume', 0)), 
            reverse=True
        )

        # 4. Ä°lk 40 tanesini "GÃ¼venli Havuz" olarak belirle (Performans SÄ±nÄ±rÄ±)
        safe_pool = sorted_pairs[:40] if len(sorted_pairs) > 40 else sorted_pairs

        # 5. KullanÄ±cÄ±nÄ±n istediÄŸi 'count' kadarÄ±nÄ± bu 40 iÃ§inden al
        final_count = min(count, len(safe_pool))
        final_symbols = [t['symbol'] for t in safe_pool[:final_count]]

        # EÄŸer hiÃ§ sembol kalmadÄ±ysa fallback
        if not final_symbols:
            return ["BTCUSDT", "ETHUSDT", "BNBUSDT"]
            
        return final_symbols

    except Exception as e:
        logger.error(f"âŒ get_top_volume_symbols hatasÄ±: {e}")
        return ["BTCUSDT", "ETHUSDT", "BNBUSDT"]  # Hata anÄ±nda fallback





   
async def fetch_data_for_pipeline(symbol, metric_defs, interval="1h", limit=500):
    # EÄŸer limit belirtilmezse Binance varsayÄ±lan olarak az veri gÃ¶nderebilir
    # Metriklerin "Ä±sÄ±nmasÄ±" iÃ§in en az 100 bar Ã§ekmelisiniz

    is_single = isinstance(symbol, str)
    symbols = [symbol] if is_single else symbol

    fetcher = BinanceDataFetcher()
    await fetcher.initialize()

    # 1) TÃ¼m metriklerden endpoint â†’ param factory Ã§Ä±kar
    endpoint_factories = {}
    for mdef in metric_defs.values():
        for ep, factory in mdef.get("endpoint_params", {}).items():
            endpoint_factories[ep] = factory

    # 2) Her symbol iÃ§in parametreleri oluÅŸtur
    tasks = {}
    for sym in symbols:
        params_for_symbol = {
            ep: factory(sym, interval, limit)
            for ep, factory in endpoint_factories.items()
        }

        tasks[sym] = asyncio.create_task(
            fetcher.fetch_all_for_symbol(sym, params_for_symbol)
        )

    # 3) SonuÃ§larÄ± topla
    results = {}
    for sym, task in tasks.items():
        try:
            df = await task
            results[sym] = df
        except Exception as e:
            logger.error(f"Failed fetch for {sym}: {e}")
            results[sym] = pd.DataFrame()

    return results[symbol] if is_single else results


def _klines_to_dataframe(klines: List, symbol: str) -> pd.DataFrame:
    if not klines:
        logger.warning(f"Klines empty for {symbol}")
        return pd.DataFrame()
    
    df = pd.DataFrame(klines, columns=[
        'timestamp', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_volume', 'trades', 'taker_buy_base',
        'taker_buy_quote', 'ignore'
    ])
    
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce', utc=True)
    df.set_index('timestamp', inplace=True)
    
    # HEP SINIR NUMERIC KOLONLAR
    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
    for c in numeric_cols:
        df[c] = pd.to_numeric(df[c], errors='coerce')
    
    # taker_buy_base ve taker_buy_quote da numeric olmalÄ±
    if 'taker_buy_base' in df.columns:
        df['taker_buy_base'] = pd.to_numeric(df['taker_buy_base'], errors='coerce')
    if 'taker_buy_quote' in df.columns:
        df['taker_buy_quote'] = pd.to_numeric(df['taker_buy_quote'], errors='coerce')
    
    # trades ve close_time integer
    if 'trades' in df.columns:
        df['trades'] = pd.to_numeric(df['trades'], errors='coerce').fillna(0).astype(int)
    if 'close_time' in df.columns:
        df['close_time'] = pd.to_numeric(df['close_time'], errors='coerce').fillna(0).astype(int)
    
    df['symbol'] = symbol
    
    # DEBUG: Tip kontrolÃ¼
    logger.debug(f"DataFrame dtypes after conversion:\n{df.dtypes}")
    
    return df


    
def _endpoint_to_dataframe(data: Any, symbol: str, endpoint_name: str) -> pd.DataFrame:
    """
    Generic normalizer for endpoints other than klines.
    Tries to infer timestamp column -> index; otherwise returns table with 'value__endpoint' if scalar list given.
    """
    if data is None:
        return pd.DataFrame()
    # If data already a DataFrame-like
    if isinstance(data, pd.DataFrame):
        df = data.copy()
        if 'timestamp' in df.columns:
            try:
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce', utc=True)
                df.set_index('timestamp', inplace=True)
            except Exception:
                pass
        df['symbol'] = symbol
        return df

    
    # If list of dicts
    if isinstance(data, list):
        try:
            df = pd.DataFrame(data)

            # -------------------------------
            # ðŸ”¥ BINANCE OPEN INTEREST PATCH
            # -------------------------------
            if endpoint_name == "open_interest_hist":
                if "sumOpenInterest" in df.columns:
                    df["open_interest"] = pd.to_numeric(
                        df["sumOpenInterest"], errors="coerce"
                    )
            # -------------------------------

            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(
                    df['timestamp'], unit='ms', errors='coerce', utc=True
                )
                df.set_index('timestamp', inplace=True)

            df['symbol'] = symbol
            return df

        except Exception:
            pass



    # If a dict with keys -> try to create series
    if isinstance(data, dict):
        try:
            # flatten scalar dicts to DataFrame with single timestamp=now
            s = pd.Series(data)
            df = pd.DataFrame([s])
            df.index = pd.to_datetime([pd.Timestamp.utcnow()])
            df['symbol'] = symbol
            return df
        except Exception:
            pass

    # If scalar or unknown -> return small DF with "value"
    try:
        return pd.DataFrame([{ 'value': data, 'symbol': symbol }], index=[pd.Timestamp.utcnow()])
    except Exception:
        return pd.DataFrame()

# ------------------------------------------------------------
# Verify metric definitions (unchanged)
# ------------------------------------------------------------
def verify_metric_definitions(metric_defs: Dict[str, Dict]) -> Dict[str, bool]:
    out = {}
    for name, d in metric_defs.items():
        ok = isinstance(d, dict) and d.get("function") is not None and d.get("execution_type") in ("sync", "async")
        out[name] = bool(ok)
        if not ok:
            logger.debug(f"Metric definition invalid: {name}")
    return out

def get_metric_metadata(metric_defs: Dict) -> Dict[str, Dict]:
    meta = {}
    for name, d in metric_defs.items():
        if not d:
            continue
        meta[name] = {
            "data_model": d.get("data_model", "unknown"),
            "execution_type": d.get("execution_type", "unknown"),
            "category": d.get("metadata", {}).get("category", "unknown"),
            "module": d.get("metadata", {}).get("module_name", "unknown"),
        }
    return meta

# ------------------------------------------------------------
# Single-symbol pipeline (updated order: resolve defs -> fetch endpoints -> calc)
# ------------------------------------------------------------

async def _run_single_pipeline(
    symbol: str,
    requested_scores: List[str],
    raw_df: Optional[pd.DataFrame] = None,
    interval: str = "1h",
    limit: int = 500
) -> Dict[str, Any]:

    logger.info(f"Pipeline start: {symbol}")

    # -------------------------------------------------
    # 1) Resolve required metrics
    # -------------------------------------------------
    score_to_metrics = resolve_scores_to_metrics(requested_scores, COMPOSITES, MACROS)
    all_required_metrics = sorted(
        {m for metrics in score_to_metrics.values() for m in metrics}
    )

    resolver = get_default_resolver()
    metric_defs = resolver.resolve_multiple_definitions(all_required_metrics)


    logger.debug(f"All required metrics: {all_required_metrics}")
    logger.debug(f"Metric defs keys: {list(metric_defs.keys())}")


    # validate & filter invalid metric defs
    valid_map = verify_metric_definitions(metric_defs)
    
    logger.debug(f"Valid metrics: {[k for k, v in valid_map.items() if v]}")
    logger.debug(f"Invalid metrics: {[k for k, v in valid_map.items() if not v]}")
    
    
    metric_defs = {k: v for k, v in metric_defs.items() if valid_map.get(k)}

    if not metric_defs:
        return {"error": "No valid metric definitions", "symbol": symbol}

    # -------------------------------------------------
    # 2) Collect required endpoints (metadata only)
    # -------------------------------------------------
    required_endpoints_set: Set[str] = set()

    for m_info in metric_defs.values():
        for ep in m_info.get("required_endpoints", []) or []:
            required_endpoints_set.add(ep)

    # Heuristic: OHLCV ihtiyacÄ± varsa klines garanti
    if "klines" not in required_endpoints_set:
        for m_info in metric_defs.values():
            req_cols = m_info.get("required_columns", []) or []
            if any(c in ("open", "high", "low", "close", "volume", "returns") for c in req_cols):
                required_endpoints_set.add("klines")
                break

    required_endpoints = sorted(required_endpoints_set)

    # -------------------------------------------------
    # 3) Fetch data (merged)
    # -------------------------------------------------
    if raw_df is None:
        try:
            merged_df = await fetch_data_for_pipeline(
                symbol, metric_defs, interval, limit
            )
        except Exception as e:
            logger.error(f"Fetch failed for {symbol}: {e}")
            return {"error": f"Data fetch failed: {e}", "symbol": symbol}
    else:
        merged_df = raw_df

    if merged_df is None or merged_df.empty:
        return {"error": "No data", "symbol": symbol}

    # âŒ GLOBAL COLUMN NORMALIZATION YOK
    # prepare_data + required_columns tek doÄŸru yol

    # -------------------------------------------------
    # 4) Calculate metrics
    # -------------------------------------------------
    metric_results = await calculate_metrics(
        merged_df,
        metric_defs,
        max_workers=_DEFAULT_MAX_WORKERS
    )

    # -------------------------------------------------
    # 5) Composite & macro scores
    # -------------------------------------------------
    composite_scores = await calculate_formula_scores(metric_results, COMPOSITES)
    macro_scores = await calculate_formula_scores(composite_scores, MACROS)

    # -------------------------------------------------
    # 6) Final score assembly
    # -------------------------------------------------
    final_scores: Dict[str, float] = {}

    for s in requested_scores:
        if s in composite_scores:
            final_scores[s] = composite_scores[s]
        elif s in macro_scores:
            final_scores[s] = macro_scores[s]
        elif s in metric_results:
            final_scores[s] = metric_results[s]
        else:
            final_scores[s] = float("nan")

    result = {
        "symbol": symbol,
        "timestamp": datetime.utcnow().isoformat(),
        "scores": final_scores,
        "metrics": metric_results,
        "composites": composite_scores,
        "macros": macro_scores,
        "metadata": {
            "metrics_count": len(metric_results),
            "valid_metrics": list(metric_results.keys()),
            "metric_defs_summary": get_metric_metadata(metric_defs),
            "required_endpoints": required_endpoints,
        },
    }

    logger.info(f"Pipeline done: {symbol}")
    return result


# ------------------------------------------------------------
# Public run_pipeline (unchanged behavior, supports single/batch)
# ------------------------------------------------------------
async def run_pipeline(symbol: Union[str, List[str]], requested_scores: List[str], raw_data: Optional[Union[pd.DataFrame, Dict[str, pd.DataFrame]]] = None, **kwargs) -> Union[Dict[str, Any], Dict[str, Dict[str, Any]]]:
    if isinstance(symbol, list):
        raw_map = raw_data if isinstance(raw_data, dict) else {}
        tasks = [asyncio.create_task(_run_single_pipeline(sym, requested_scores, raw_map.get(sym), **kwargs)) for sym in symbol]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        return {sym: res for sym, res in zip(symbol, results)}
    else:
        return await _run_single_pipeline(symbol, requested_scores, raw_data, **kwargs)

def run_pipeline_sync(symbol: Union[str, List[str]], requested_scores: List[str], raw_data: Optional[Union[pd.DataFrame, Dict[str, pd.DataFrame]]] = None, timeout: int = 30, **kwargs) -> Union[Dict[str, Any], Dict[str, Dict[str, Any]]]:
    try:
        return asyncio.run(run_pipeline(symbol, requested_scores, raw_data, **kwargs))
    except Exception as e:
        logger.error(f"run_pipeline_sync failed: {e}")
        return {"error": str(e), "symbol": symbol}

# ------------------------------------------------------------
# Debug helpers etc. (unchanged)
# ------------------------------------------------------------
async def debug_metric_calculation(metric_name: str, data: pd.DataFrame) -> Dict:
    resolver = get_default_resolver()
    def_info = resolver.resolve_metric_definition(metric_name)
    info = {
        "metric_name": metric_name,
        "data_model": def_info.get("data_model"),
        "execution_type": def_info.get("execution_type"),
        "required_columns": def_info.get("required_columns", []),
        "available_columns": list(data.columns),
        "normalization": def_info.get("normalization", {}),
        "category": def_info.get("metadata", {}).get("category"),
    }
    func = def_info.get("function")
    if func:
        params = def_info.get("default_params", {})
        try:
            if def_info.get("execution_type") == "async":
                raw = await func(data, **params)
            else:
                raw = func(data, **params)
            info["raw_result_type"] = type(raw).__name__
            info["final_value"] = extract_final_value(raw, metric_name)
        except Exception as e:
            info["error"] = str(e)
    return info

def get_system_status() -> Dict:
    resolver = get_default_resolver()
    all_metrics = resolver.get_available_metrics()
    sample = {}
    for m in ["ema", "rsi", "macd"]:
        try:
            d = resolver.resolve_metric_definition(m)
            sample[m] = {"data_model": d.get("data_model"), "execution_type": d.get("execution_type")}
        except Exception:
            pass
    return {
        "total_metrics": len(all_metrics),
        "sample_metrics": sample,
        "default_data_model": DEFAULT_DATA_MODEL,
        "executor_workers": _DEFAULT_MAX_WORKERS
    }


# -------
async def test_open_interest_hist_raw():
    from utils.binance_api.binance_a import BinanceAggregator

    agg = await BinanceAggregator.get_instance()

    print("ðŸ‘‰ open_interest_hist RAW TEST START")

    data = await agg.get_public_data(
        endpoint_name="open_interest_hist",
        symbol="BTCUSDT",
        period="1h",
        limit=5
    )

    print("ðŸ‘‰ RESPONSE TYPE:", type(data))
    print("ðŸ‘‰ RESPONSE:", data)
